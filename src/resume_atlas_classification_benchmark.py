# -*- coding: utf-8 -*-
"""resume_atlas_classification_benchmark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xSCDKLilp7eLF6m8YC35J2PEKXSO4UAa

# R√©sum√© Atlas Classification Benchmark

This Colab notebook fine‚Äëtunes or trains several text‚Äëclassification models on the same train/validation/test splits of the [R√©sum√©‚ÄØAtlas](https://huggingface.co/datasets/ahmedheakl/resume-atlas) dataset used in your previous experiment and reports comparable metrics.

**Models**
- TF‚ÄëIDF¬†+¬†SVM
- FastText (Wiki vectors)
- CareerBERT‚Äëbase
- CareerBERT‚Äëlarge
- RoBERTa‚ÄëDA

At the end, you‚Äôll get a concise table with *Top‚Äëk* accuracies (1/3/5/10), Top‚Äë1 accuracy, F1‚Äëmacro, Precision‚Äëmacro, and Recall‚Äëmacro for head‚Äëto‚Äëhead comparison.

## üîß¬†Install libraries
"""

# üîß  –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–æ–≤–Ω–æ —Ç–æ, —á—Ç–æ –Ω—É–∂–Ω–æ
!pip install -U pip setuptools wheel

!pip install -U "numpy==1.26.4" "fasttext==0.9.2" "sentencepiece>=0.1.99"

!pip install peft==0.10.0

!pip -q install -U "transformers>=4.41" "datasets>=2.19" "evaluate>=0.4" \
                  "sentencepiece" "accelerate>=0.31" \
                  "nltk>=3.9"

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

"""## üìö¬†Imports & helpers"""

import numpy as np, pandas as pd, torch, re, string
#import fasttext
from datasets import load_dataset, DatasetDict, concatenate_datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.calibration import CalibratedClassifierCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from tqdm.auto import tqdm
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          TrainingArguments, Trainer)
import random, os
from pathlib import Path
tqdm.pandas()

def set_all_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
set_all_seeds()



"""## üìë¬†Load & preprocess R√©sum√©¬†Atlas"""

RAW = load_dataset('ahmedheakl/resume-atlas')
full = RAW['train'] if 'train' in RAW else concatenate_datasets(list(RAW.values()))

TEXT_COLS  = ['text','resume_text','ocr_text','content']
text_col   = next(c for c in full.column_names if c.lower() in TEXT_COLS)
label_col  = 'Category'

# 70/10/20 stratified split identical to original notebook
y = np.array(full[label_col]); idx = np.arange(len(full))
tr, tmp, y_tr, y_tmp = train_test_split(idx, y, test_size=0.3,
                                        stratify=y, random_state=42)
val, test, _, _ = train_test_split(tmp, y_tmp, test_size=2/3,
                                   stratify=y_tmp, random_state=42)
splits = DatasetDict(train=full.select(tr.tolist()),
                     validation=full.select(val.tolist()),
                     test=full.select(test.tolist()))

label_list = sorted(set(splits['train'][label_col]))
label2id = {l:i for i,l in enumerate(label_list)}
id2label = {i:l for l,i in label2id.items()}
num_labels = len(label_list)

def add_numeric_label(example):
    example['label'] = label2id[example[label_col]]
    return example
splits = splits.map(add_numeric_label, remove_columns=[])
print(splits)

"""## üìê¬†Metric helpers (Top‚Äëk, F1‚Äëmacro, ‚Ä¶)"""

def topk(prob: np.ndarray, y: np.ndarray, ks=(1,3,5,10)):
    idx = np.argsort(-prob, 1)
    return {f'Top-{k}': float((y[:,None] == idx[:,:k]).any(1).mean()) for k in ks}

def compute_metrics(probs: np.ndarray, y_true: np.ndarray):
    preds = probs.argmax(1)
    top_metrics = topk(probs, y_true)
    acc = accuracy_score(y_true, preds)
    pma, rma, f1ma, _ = precision_recall_fscore_support(
        y_true, preds, average='macro', zero_division=0)
    return {**top_metrics,
            'Top-1': float(acc),
            'F1-macro': float(f1ma),
            'Precision-macro': float(pma),
            'Recall-macro': float(rma)}

"""## üìù¬†TF‚ÄëIDF¬†+¬†Linear¬†SVM"""

from scipy.special import softmax
vec = TfidfVectorizer(stop_words='english', max_features=50_000, ngram_range=(1,2))
svm = LinearSVC()
clf = CalibratedClassifierCV(svm)  # enables predict_proba
X_train = vec.fit_transform(splits['train'][text_col])
y_train = splits['train']['label']
clf.fit(X_train, y_train)

X_test = vec.transform(splits['test'][text_col])
probs = clf.predict_proba(X_test)
metrics_svm = compute_metrics(probs, np.array(splits['test']['label']))
print(metrics_svm)

"""## üèÉ‚Äç‚ôÇÔ∏è¬†FastText (Wiki) + Logistic Regression"""

# Download pretrained wiki vectors (English)
import fasttext.util, os, urllib.request, zipfile, pathlib, io, gzip, shutil, subprocess, sys, textwrap
ft_path = 'cc.en.300.bin'
if not Path(ft_path).exists():
    !wget -q https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz
    !gunzip cc.en.300.bin.gz
ft_model = fasttext.load_model(ft_path)

def embed_ft(texts):
    return np.vstack([ft_model.get_sentence_vector(t) for t in texts])

X_train_ft = embed_ft(splits['train'][text_col])
X_test_ft  = embed_ft(splits['test'][text_col])

lr = LogisticRegression(max_iter=1000, n_jobs=-1, multi_class='multinomial')
lr.fit(X_train_ft, y_train)
probs_ft = lr.predict_proba(X_test_ft)
metrics_ft = compute_metrics(probs_ft, np.array(splits['test']['label']))
print(metrics_ft)

"""## ü§ñ¬†Helper to fine‚Äëtune transformer models"""

def finetune_transformer(model_ckpt: str, output_dir: str, epochs: int = 2, batch: int = 8,
                         lr: float = 2e-5):
    tok = AutoTokenizer.from_pretrained(model_ckpt)
    def tokenize(batch):
        return tok(batch[text_col], truncation=True, padding='max_length', max_length=256)
    tok_splits = splits.map(tokenize, batched=True)
    tok_splits.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

    model = AutoModelForSequenceClassification.from_pretrained(
        model_ckpt, num_labels=num_labels, id2label=id2label, label2id=label2id)

    args = TrainingArguments(
        output_dir=output_dir, eval_strategy='epoch',
        learning_rate=lr, per_device_train_batch_size=batch,
        per_device_eval_batch_size=batch, num_train_epochs=epochs,
        weight_decay=0.01, logging_steps=100, save_strategy='no')
    trainer = Trainer(model=model, args=args,
                      train_dataset=tok_splits['train'],
                      eval_dataset=tok_splits['validation'])
    trainer.train()

    preds = trainer.predict(tok_splits['test'])
    probs = torch.softmax(torch.tensor(preds.predictions), dim=-1).numpy()
    return compute_metrics(probs, preds.label_ids)

"""### CareerBERT‚Äëbase"""

careerbert_base_ckpt = 'lwolfrum2/careerbert-g'  # replace with correct base checkpoint if different
metrics_cb_base = finetune_transformer(careerbert_base_ckpt, 'careerbert_base')
print(metrics_cb_base)

"""### CareerBERT‚Äëlarge"""

careerbert_large_ckpt = 'lwolfrum2/careerbert-jg'  # replace with correct large checkpoint
metrics_cb_large = finetune_transformer(careerbert_large_ckpt, 'careerbert_large')
print(metrics_cb_large)

"""### RoBERTa‚ÄëDA"""

# --- 1. –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä ---
roberta_da_ckpt = "mediabiasgroup/da-roberta-babe-ft"

# --- 2. –¥–æ–ø—É—Å—Ç–∏—Ç—å –∑–∞–º–µ–Ω—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–æ–ª–æ–≤—ã ---
def finetune_transformer(model_ckpt, output_dir, epochs=2, batch=8, lr=2e-5):
    tok = AutoTokenizer.from_pretrained(model_ckpt)

    def tokenize(batch):
        return tok(batch[text_col],
                   truncation=True, padding='max_length', max_length=256)

    tok_splits = splits.map(tokenize, batched=True)
    tok_splits.set_format(type='torch',
                          columns=['input_ids', 'attention_mask', 'label'])

    model = AutoModelForSequenceClassification.from_pretrained(
        model_ckpt,
        num_labels=num_labels,
        id2label=id2label,
        label2id=label2id,
        ignore_mismatched_sizes=True   # <-- –≥–ª–∞–≤–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
    )

    args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy='epoch',
        learning_rate=lr,
        per_device_train_batch_size=batch,
        per_device_eval_batch_size=batch,
        num_train_epochs=epochs,
        weight_decay=0.01,
        logging_steps=100,
        save_strategy='no',
    )
    trainer = Trainer(model=model, args=args,
                      train_dataset=tok_splits['train'],
                      eval_dataset=tok_splits['validation'])
    trainer.train()

    preds = trainer.predict(tok_splits['test'])
    probs = torch.softmax(torch.tensor(preds.predictions), dim=-1).numpy()
    return compute_metrics(probs, preds.label_ids)

#roberta_da_ckpt = 'Datadave09/DA-RoBERTa'
metrics_roberta_da = finetune_transformer(roberta_da_ckpt, 'roberta_da')
print(metrics_roberta_da)