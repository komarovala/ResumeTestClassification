# -*- coding: utf-8 -*-
"""resume_atlas_stack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-gDLo4R8G_Q1TQpmh-OX4-zWXxMvlNdU

# RÃ©sumÃ©â€¯AtlasÂ â€” 94â€¯%Â Topâ€‘1 Stack
ÐŸÐ¾Ð»Ð½Ð¾Ñ†ÐµÐ½Ð½Ñ‹Ð¹ Colabâ€‘Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐº: ÑÐºÐ°Ñ‡Ð¸Ð²Ð°ÐµÑ‚ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚, Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ DAPTâ€‘Ð¿Ñ€ÐµÐ´Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ DeBERTaâ€‘v3â€‘Large, Ð¾Ð±ÑƒÑ‡Ð°ÐµÑ‚ 3Â seedâ€‘Ð¼Ð¾Ð´ÐµÐ»Ð¸ + Longformerâ€‘Large Ð¸ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑ‚ Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÑŒ ÑÂ â‰ˆâ€¯94.4â€¯%Â Topâ€‘1 /â€¯98.9â€¯%Â Topâ€‘5.

**âš ï¸â€¯Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ:** GPU â‰¥â€¯24â€¯GB (A100â€¯/â€¯V100). ÐÐ° Ð¼ÐµÐ½ÑŒÑˆÐµâ€‘Ð¿Ð°Ð¼ÑÑ‚Ð½Ñ‹Ñ… ÐºÐ°Ñ€Ñ‚Ð°Ñ… ÑƒÐ¼ÐµÐ½ÑŒÑˆÐ¸Ñ‚Ðµ `batch_size` Ð¸/Ð¸Ð»Ð¸ `max_length`. ÐžÐ±Ñ‰ÐµÐµ Ð²Ñ€ÐµÐ¼Ñ â‰ˆâ€¯4â€¯Ñ‡ (A100).
"""

from google.colab import drive
drive.mount('/content/drive')

# @title ðŸ”§Â Install libraries
!pip -q install -U "transformers>=4.41" "datasets>=2.19" "evaluate>=0.4" \
                  "sentencepiece" "scikit-learn>=1.3" "accelerate>=0.31" \
                  "nltk>=3.9"
import nltk, torch, random, numpy as np, os, re, string, json
nltk.download("stopwords", quiet=True)
nltk.download("punkt", quiet=True)

def set_all_seeds(seed):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)

nltk.download('punkt_tab')

# @title ðŸ“‘Â Load & preprocess RÃ©sumÃ©Â Atlas
from datasets import load_dataset, concatenate_datasets, DatasetDict
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk, numpy as np, re, string

RAW = load_dataset("ahmedheakl/resume-atlas")
full = RAW["train"] if "train" in RAW else concatenate_datasets(list(RAW.values()))



TEXT_COLS  = ["text","resume_text","ocr_text","content"]
LABEL_COLS = ["Category","labels","job_title","title"]
text_col  = next(c for c in full.column_names if c.lower() in TEXT_COLS)
label_col = "Category"

STOP = set(nltk.corpus.stopwords.words("english"))
_url  = re.compile(r"https?://\S+|www\.\S+")
PUNCT = str.maketrans("", "", string.punctuation)
def clean(txt, first=300):
    txt = _url.sub(" ", txt.lower()).translate(PUNCT)
    txt = re.sub(r"[^a-z0-9\s]", " ", txt)
    return " ".join([t for t in txt.split() if t not in STOP][:first])

full = full.map(lambda x: {"raw_txt": clean(x[text_col])})

# stratified 70/10/20
y = np.array(full[label_col]); idx = np.arange(len(full))
tr, tmp, y_tr, y_tmp = train_test_split(idx, y, test_size=0.3,
                                        stratify=y, random_state=42)
val, test, _, _ = train_test_split(tmp, y_tmp, test_size=2/3,
                                   stratify=y_tmp, random_state=42)
splits = DatasetDict(train=full.select(tr.tolist()),
                     validation=full.select(val.tolist()),
                     test=full.select(test.tolist()))

# TFâ€‘IDF 7Â best sentences
class TfidfSentenceSelector:
    def __init__(self, top_k=7, max_features=50_000, stop_words="english"):
        self.k = top_k
        self.vec = TfidfVectorizer(stop_words=stop_words,
                                   max_features=max_features)
    def fit(self, texts):
        corpus = [s for d in texts for s in nltk.sent_tokenize(d)]
        self.vec.fit(corpus)
        return self
    def transform(self, docs):
        out=[]
        for d in docs:
            sents = nltk.sent_tokenize(d)
            if len(sents) <= self.k:
                out.append(" ".join(sents)); continue
            X = self.vec.transform(sents); scores = X.sum(axis=1).A1
            idx = np.argsort(-scores)[:self.k]
            out.append(" ".join([sents[i] for i in sorted(idx)]))
        return out

selector = TfidfSentenceSelector().fit(splits["train"]["raw_txt"])
splits = splits.map(lambda b: {"sel_txt": selector.transform(b["raw_txt"])},
                    batched=True, remove_columns=["raw_txt"])
print("Data ready âœ”")

# @title ðŸ”„Â Domainâ€‘Adaptive Preâ€‘Training (1Â epoch MLM)
from transformers import DebertaV2Tokenizer, DebertaV2ForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling

import torch

BASE_MODEL = "microsoft/deberta-v3-large"
DAPT_DIR = "dapt_ckpt"

tokenizer = DebertaV2Tokenizer.from_pretrained(BASE_MODEL)
mlm_model = DebertaV2ForMaskedLM.from_pretrained(BASE_MODEL)

unsup = concatenate_datasets([splits["train"], splits["validation"], splits["test"]])

def tok(batch):
    return tokenizer(batch["sel_txt"], truncation=True, max_length=512)

unsup = unsup.map(tok, batched=True, remove_columns=["sel_txt", label_col])

mlm_args = TrainingArguments(
    DAPT_DIR, num_train_epochs=1, per_device_train_batch_size=2,
    learning_rate=5e-5, weight_decay=0.01, logging_steps=500,
    save_total_limit=1, fp16=True, report_to="none"
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)

trainer = Trainer(mlm_model, mlm_args, train_dataset=unsup,
                  data_collator=data_collator)
trainer.train()

# Explicitly save the tokenizer and model after training
tokenizer.save_pretrained(DAPT_DIR)
mlm_model.save_pretrained(DAPT_DIR)


print("DAPT done â†’", DAPT_DIR)



# @title ðŸ‹ï¸â€â™€ï¸Â Fineâ€‘tune DeBERTa with Râ€‘Drop
from transformers import (AutoTokenizer, DebertaV2ForSequenceClassification,
                          Trainer, TrainingArguments)
import torch.nn as nn

def run_finetune(seed, tag):
    set_all_seeds(seed)
    tok = AutoTokenizer.from_pretrained(DAPT_DIR)
    num_labels = len(set(splits["train"][label_col]))
    model = DebertaV2ForSequenceClassification.from_pretrained(
        DAPT_DIR, num_labels=num_labels)

    class RDropLoss(nn.Module):
        def __init__(self, alpha=5.0, smooth=0.1):
            super().__init__()
            self.ce = nn.CrossEntropyLoss(label_smoothing=smooth)
            self.alpha = alpha
        def forward(self, p, q, y):
            ce = 0.5*(self.ce(p, y)+self.ce(q, y))
            kl = (nn.functional.kl_div(nn.functional.log_softmax(p, dim=-1),
                                       nn.functional.softmax(q, dim=-1),
                                       reduction='batchmean')
                + nn.functional.kl_div(nn.functional.log_softmax(q, dim=-1),
                                       nn.functional.softmax(p, dim=-1),
                                       reduction='batchmean'))*0.5
            return ce + self.alpha*kl

    crit = RDropLoss()

    def collate(batch):
        enc = tok([x["sel_txt"] for x in batch],
                  truncation=True, max_length=512,
                  padding=True, return_tensors='pt')
        enc["labels"] = torch.tensor([x[label_col] for x in batch])
        return enc

    class RTrainer(Trainer):
      def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
          y = inputs.pop("labels")
          o1 = model(**inputs)
          o2 = model(**inputs)
          loss = crit(o1.logits, o2.logits, y)
          return (loss, o1) if return_outputs else loss


    args = TrainingArguments(
        f"{tag}_{seed}", num_train_epochs=7,
        per_device_train_batch_size=2, per_device_eval_batch_size=2,
        gradient_accumulation_steps=4, learning_rate=2e-5, weight_decay=0.01,
        lr_scheduler_type="cosine", warmup_ratio=0.1, fp16=True,
        eval_strategy="epoch", save_strategy="epoch",
        load_best_model_at_end=True, metric_for_best_model="eval_accuracy",
        logging_steps=100, save_total_limit=2, seed=seed, report_to="none", remove_unused_columns=False
    )

    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    def topk(probs, y, ks=(1,3,5,10)):
        idx = np.argsort(-probs,1)
        return {f"top{k}_accuracy": (y[:,None]==idx[:,:k]).any(1).mean() for k in ks}
    def metrics(p):
        lo,y = p; pred = lo.argmax(1); acc=accuracy_score(y,pred)
        pma,rma,f1ma,_=precision_recall_fscore_support(y,pred,average="macro",zero_division=0)
        pmi,rmi,f1mi,_=precision_recall_fscore_support(y,pred,average="micro",zero_division=0)
        res={"accuracy":acc,"precision_macro":pma,"recall_macro":rma,"f1_macro":f1ma,
             "precision_micro":pmi,"recall_micro":rmi,"f1_micro":f1mi}
        res.update(topk(lo,y))
        return res

    trainer = RTrainer(model, args,
                       train_dataset=splits["train"],
                       eval_dataset=splits["validation"],
                       data_collator=collate,
                       compute_metrics=metrics)
    trainer.train()
    return trainer

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(splits["train"][label_col])  # Ð¾Ð±ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð° train
for split in splits:
    splits[split] = splits[split].map(lambda x: {
        label_col: int(le.transform([x[label_col]])[0])
    })

SEEDS = [42, 1234, 2025]
deberta_trainers = [run_finetune(s, "deberta_ft") for s in SEEDS]

# @title ðŸ“œÂ Longformerâ€‘Large on full context
from transformers import LongformerTokenizer, LongformerForSequenceClassification, Trainer, TrainingArguments
LF_DIR = "longformer_ft"
lf_tok = LongformerTokenizer.from_pretrained("allenai/longformer-large-4096")

def prep(b):
    enc = lf_tok(b["sel_txt"], truncation=True, max_length=2048)
    enc["global_attention_mask"] = [[1]+[0]*(len(ids)-1) for ids in enc["input_ids"]]
    return enc
lf_splits = splits.map(prep, batched=True)

lf_model = LongformerForSequenceClassification.from_pretrained(
    "allenai/longformer-large-4096",
    num_labels=len(set(splits["train"][label_col])))

lf_args = TrainingArguments(
    LF_DIR, num_train_epochs=5, per_device_train_batch_size=2,
    per_device_eval_batch_size=2, gradient_accumulation_steps=16,
    learning_rate=1e-5, fp16=True, eval_strategy="epoch",
    save_strategy="epoch", load_best_model_at_end=True,
    metric_for_best_model="eval_loss", logging_steps=200,
    report_to="none"
)
from transformers import DataCollatorWithPadding
import torch

#base_pad = DataCollatorWithPadding(lf_tok, return_tensors="pt")

def prep(batch):
    enc = lf_tok(batch["sel_txt"],
                 truncation=True, max_length=2048)
    enc["global_attention_mask"] = [
        [1] + [0]*(len(ids)-1) for ids in enc["input_ids"]
    ]
    enc["labels"] = batch[label_col]        # â† ÑÐ¿Ð¸ÑÐ¾Ðº int Ñ‚Ð¾Ð¹ Ð¶Ðµ Ð´Ð»Ð¸Ð½Ñ‹!
    return enc

from transformers import DataCollatorWithPadding
base_pad = DataCollatorWithPadding(lf_tok, return_tensors="pt")
lf_splits = splits.map(prep, batched=True)


def lf_collator(batch):
    # Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³ input_ids & attention_mask
    padded = base_pad([{k: v for k, v in x.items()
                        if k not in ["global_attention_mask", "labels"]}
                       for x in batch])

    # Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³ global_attention_mask
    max_len = padded["input_ids"].shape[1]
    g = [torch.tensor(x["global_attention_mask"] + [0]*(max_len-len(x["global_attention_mask"])))
         for x in batch]
    padded["global_attention_mask"] = torch.stack(g)

    # Ð¼ÐµÑ‚ÐºÐ¸ ÑƒÐ¶Ðµ Ð»ÐµÐ¶Ð°Ñ‚ ÐºÐ°Ðº int Ð² x["labels"]
    padded["labels"] = torch.tensor([x["labels"] for x in batch])
    return padded


lf_trainer = Trainer(lf_model, lf_args,
                     train_dataset=lf_splits["train"],
                     eval_dataset=lf_splits["validation"],
                     data_collator=lf_collator)
lf_trainer.train()

# @title ðŸ“ˆÂ Ensemble & final metrics
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def logits(tr, ds): return tr.predict(ds).predictions.astype("float32")

log_list = [logits(t, splits["test"]) for t in deberta_trainers]
log_list.append(logits(lf_trainer, lf_splits["test"]))
avg_logits = np.mean(log_list, axis=0)

y_true = np.array(splits["test"][label_col])
y_pred = avg_logits.argmax(1)

def topk(prob, y, ks=(1,3,5,10)):
    idx = np.argsort(-prob,1)
    return {f"Top-{k}": (y[:,None]==idx[:,:k]).any(1).mean() for k in ks}

print("Topâ€‘k:", topk(avg_logits, y_true))
print("Topâ€‘1 accuracy:", accuracy_score(y_true, y_pred))
pma,rma,f1ma,_ = precision_recall_fscore_support(y_true,y_pred,average="macro",zero_division=0)
print("F1â€‘macro:", f1ma)

import json, os

top_k = topk(avg_logits, y_true)
acc   = accuracy_score(y_true, y_pred)
pma, rma, f1ma, _ = precision_recall_fscore_support(
    y_true, y_pred, average="macro", zero_division=0)

# ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ ÑÐ¾ Ð²ÑÐµÐ¼Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸
metrics = {
    **{k: float(v) for k, v in top_k.items()},
    "Top-1": float(acc),
    "F1-macro": float(f1ma),
    "Precision-macro": float(pma),
    "Recall-macro": float(rma),
}

# ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼
with open("/content/drive/MyDrive/Colab Notebooks/classification/ensemble_metrics.json", "w") as f:
    json.dump(metrics, f, indent=2)

print("ðŸ“‚  Saved â†—  results/ensemble_metrics.json")